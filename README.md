
# EnergyAIâ€‘TimeSeriesLab

**Endâ€‘toâ€‘end deepâ€‘learning pipeline for smartâ€‘building power data**  
Data cleaning â–¶ feature engineering â–¶ LSTM/CNNâ€‘LSTM/Transformer â–¶ anomaly detection â–¶ transfer learning.

---

## ğŸ”¥ Key Dataâ€‘Science Highlights


| Skill | How itâ€™s demonstrated here |
|-------|---------------------------|
| **Timeâ€‘series feature engineering** | Cyclical encodings (`sin/cos`), holiday/weekend flags, Isolationâ€‘Forest outlier capping, hourâ€‘level resampling. |
| **Model development at scale** | LSTM, CNNâ€‘LSTM, and Transformer architectures **Bayesianâ€‘tuned** with KerasÂ Tuner. |
| **Rigorous evaluation** | RMSE, MAE, RÂ², **CVâ€‘RMSE**, multiâ€‘horizon error curves, hourâ€‘ofâ€‘day error maps. |
| **Transfer learning** | 72â€¯% RMSE drop by fineâ€‘tuning the champion model on a lowâ€‘data floor. |
| **Anomaly analytics** | MeanÂ +Â 3â€¯Ïƒ threshold, 51 anomalies characterised by hour & weekday distributions. |
| **Domainâ€‘specific KPIs** | EUI (205.4â€¯kWhâ€¯mâ»Â²â€¯yrâ»Â¹), load factor, peak/base loads, daily archetype profiles. |
| **Reproducibility** | Global `np`/`tf` seeds, Conda `environment.yml`, exact hyperâ€‘params logged, notebookâ€¯+â€¯HTML render. |

---

## ğŸ“‚ Repository structure
```text
EnergyAI-TimeSeriesLab/
â”œâ”€â”€ EnergyAI-TimeSeriesLab.ipynb   # main notebook (fully explained, runnable)
â”œâ”€â”€ EnergyAI-TimeSeriesLab.html    # static render for quick browsing
â”œâ”€â”€ data/                          # (add Floor1â€‘7 CSVs here, .gitignored for size)
â”œâ”€â”€ environment.yml                # conda spec for Appleâ€‘Silicon + TensorFlowâ€‘Metal
â””â”€â”€ README.md
```

---

## ğŸš€ Quick start
```bash
# 1ï¸âƒ£  Clone + set up env
git clone https://github.com/mnikoopayan/EnergyAI-TimeSeriesLab.git
cd EnergyAI-TimeSeriesLab
conda env create -f environment.yml
conda activate tf_m1      # same env used in the study

# 2ï¸âƒ£  Drop the seven Floor*.csv files into ./data
#     (Dataset originally part of the CUâ€‘BEMS public release.)

# 3ï¸âƒ£  Launch the lab
jupyter lab EnergyAI-TimeSeriesLab.ipynb
```

All code blocks are **cellâ€‘byâ€‘cell runnable**; each section is selfâ€‘contained so you can jump straight to modelling or anomaly analytics.

---

## ğŸ“Š Key results

### Oneâ€‘stepâ€‘ahead forecasting (test set, FloorÂ 6)
| Model | RMSE (kWh) | MAE (kWh) | RÂ² | CVâ€‘RMSE (%) | TrainÂ time |
|-------|-----------:|----------:|---:|------------:|-----------:|
| **TunedÂ LSTM** | **3.62** | 2.17 | 0.986 | 16.5 | 203â€¯s |
| CNNâ€‘LSTM | 3.71 | 2.18 | 0.985 | 16.9 | 314â€¯s |
| Transformer | 4.29 | 2.42 | 0.981 | 19.6 | 422â€¯s |

### Multiâ€‘horizon champion (tuned LSTM)
| Horizon | RMSE | MAE | RÂ² | NMAE % |
|---------|-----:|----:|---:|-------:|
| 1â€¯h | 4.22 | 2.59 | 0.981 | 11.9 |
| 3â€¯h | 4.89 | 2.92 | 0.975 | 13.4 |
| 6â€¯h | 4.92 | 3.02 | 0.974 | 13.8 |
| 12â€¯h | 6.71 | 3.56 | 0.952 | 16.4 |
| 24â€¯h | 8.13 | 4.00 | 0.929 | 18.4 |

### Transfer learning (FloorÂ 4, lowâ€‘data JulyÂ 2018)
| Strategy | Test RMSE |
|----------|----------:|
| Train from scratch | 20.78â€¯kWh |
| **Fineâ€‘tuned champion** | **5.76â€¯kWh** |

> **Î”â€¯=â€¯â€‘72â€¯%** error â€” showcases how pretrained sequence encoders cut data requirements.

---

## ğŸ§ Project story
1. **Data wrangling** â€“ 689â€¯128 rowsâ€¯Ã—â€¯30 sensor channels (per floor) cleaned; power â†’ energy via hourly means.  
2. **Feature factory** â€“ time & cyclical features, holiday flags, Isolationâ€‘Forest capping at 99.5â€¯% quantile.  
3. **Model zoo** â€“ three deepâ€‘learning families, **Bayesianâ€‘optimised** then fully trained with earlyâ€‘stopping.  
4. **Champion selection** â€“ LSTM wins on RMSE + compute cost; extended to multiâ€‘output forecaster.  
5. **Explainability hooks** â€“ predictionsâ€‘vsâ€‘actuals plots, error heatâ€‘maps, anomaly visualisations.  
6. **Crossâ€‘floor transfer** â€“ demonstrate model reuse when Phaseâ€‘II deployment data are scarce.  
7. **Building KPIs** â€“ EUI, load factor, weekday/weekend archetypes for facility ops teams.

---

## ğŸ“Œ Requirements
* PythonÂ â‰¥Â 3.10  
* TensorFlowÂ 2.16â€¯+â€¯Metal for AppleÂ Silicon (`environment.yml`)  
* kerasâ€‘tuner, scikitâ€‘learn, seaborn, pandas, matplotlib  

---

## â–¶ï¸ Next milestones
* Integrate weather covariates (dryâ€‘bulb, dewâ€‘point) via API pull.  
* Serve forecasts through FastAPI + Streamlit dashboard.  
* Extend transferâ€‘learning demo to unseen buildings (domain adaptation).

---

## âœ¨ Author
**Mohammadâ€¯Saleh NikoopayanÂ Tak** â€“ PhD candidate @â€¯NJIT | Urban Systems Tech | Dataâ€‘Science for the Built Environment  
[LinkedIn](https://www.linkedin.com/in/nikoopayan) â€¢ [GitHub](https://github.com/mnikoopayan)

*Open to collaborations and conversations on smartâ€‘building data science!*
